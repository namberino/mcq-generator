{
  "mcqs": {
    "1": {
      "câu hỏi": "Trong bảng so sánh các phương pháp xử lý Feature Correlation, phương pháp nào được mô tả có nền tảng thống kê và sử dụng ngưỡng (threshold) khách quan?",
      "lựa chọn": {
        "a": "Loại bỏ Features",
        "b": "VIF-based Removal",
        "c": "PCA",
        "d": "Autoencoder"
      },
      "đáp án": "VIF-based Removal"
    },
    "2": {
      "câu hỏi": "Trong ví dụ pipeline, bước nào được dùng để giảm ảnh hưởng của các giá trị ngoại lệ (outliers) lên các đặc trưng?",
      "lựa chọn": {
        "a": "RobustScaler",
        "b": "EditedNearestNeighbours",
        "c": "KNeighborsClassifier",
        "d": "StratifiedKFold"
      },
      "đáp án": "RobustScaler"
    },
    "3": {
      "câu hỏi": "SMOTE tạo ra các điểm thiểu số tổng hợp mới bằng cách nào?",
      "lựa chọn": {
        "a": "Sao chép ngẫu nhiên các mẫu thiểu số hiện có.",
        "b": "Nội suy giữa một mẫu thiểu số và một trong các hàng xóm thiểu số gần nhất của nó.",
        "c": "Tạo các mẫu tổng hợp từ dữ liệu lớp đa số.",
        "d": "Loại bỏ một số mẫu đa số để cân bằng dữ liệu."
      },
      "đáp án": "Nội suy giữa một mẫu thiểu số và một trong các hàng xóm thiểu số gần nhất của nó."
    },
    "4": {
      "câu hỏi": "Theo nội dung trên, phương pháp nào được đề xuất để tăng cường ảnh hưởng của lớp thiểu số trong kNN?",
      "lựa chọn": {
        "a": "Tăng giá trị k",
        "b": "Bỏ phiếu kNN có trọng số khoảng cách",
        "c": "Giảm số chiều dữ liệu",
        "d": "Sử dụng thuật toán Random Forest"
      },
      "đáp án": "Bỏ phiếu kNN có trọng số khoảng cách"
    },
    "5": {
      "câu hỏi": "Trong ví dụ về dataset mất cân bằng, tại sao thuật toán 1 (luôn trả về 0) vẫn đạt accuracy 99%?",
      "lựa chọn": {
        "a": "Vì số lượng bệnh nhân không mắc ung thư chiếm đa số trong tập dữ liệu.",
        "b": "Vì mô hình được huấn luyện tốt và học được các đặc trưng quan trọng.",
        "c": "Vì độ chính xác (accuracy) luôn phản ánh đúng chất lượng của mô hình.",
        "d": "Vì thuật toán sử dụng chỉ số precision cao để đánh giá."
      },
      "đáp án": "Vì số lượng bệnh nhân không mắc ung thư chiếm đa số trong tập dữ liệu."
    }
  },
  "validation": {
    "1": {
      "supported_by_embeddings": true,
      "max_similarity": 0.6565414071083069,
      "evidence": [
        {
          "idx": 118,
          "page": 46,
          "score": 0.6023983359336853,
          "text": "**AI VIETNAM** **aivietnam.edu.vn**\n\n\nHình 5: Ảnh hưởng feature correlation đến performance của KNN\n\n\n**7.5.3 Cách xửlý Feature Correlation**\n\n\nKhi phát hiện các features có correlation cao, chúng ta có nhiều cách tiếp cận đểxửlý vấn đềnày. Dưới\nđây là các phương pháp chính được sửdụng trong thực tế.\n\n\n**7.5.3.1 Loại bỏFeature dư thừa**\n\n\nPhương pháp đơn giản và hiệu quảnhất là loại bỏcác features có correlation cao với nhau. Thông\nthường, chúng ta giữlại feature có tương quan mạnh nhất với target variable hoặc feature có ý nghĩa\nbusiness quan trọng hơn.\n\n\n1 `import` `numpy as np`\n\n2 `import` `pandas as pd`\n\n\n3\n\n\n4 `def` `remove_correlated_features (X, threshold =0.8):`\n\n\n5 `\"\"\"`\n\n\n6 `Remove` `features` `with high` `correlation`\n\n\n7\n\n\n8 `Parameters:`\n\n\n9 `-----------`\n\n\n10 `X : array -like`\n\n\n45"
        },
        {
          "idx": 158,
          "page": 46,
          "score": 0.6023983359336853,
          "text": "**AI VIETNAM** **aivietnam.edu.vn**\n\n\nHình 5: Ảnh hưởng feature correlation đến performance của KNN\n\n\n**7.5.3 Cách xửlý Feature Correlation**\n\n\nKhi phát hiện các features có correlation cao, chúng ta có nhiều cách tiếp cận đểxửlý vấn đềnày. Dưới\nđây là các phương pháp chính được sửdụng trong thực tế.\n\n\n**7.5.3.1 Loại bỏFeature dư thừa**\n\n\nPhương pháp đơn giản và hiệu quảnhất là loại bỏcác features có correlation cao với nhau. Thông\nthường, chúng ta giữlại feature có tương quan mạnh nhất với target variable hoặc feature có ý nghĩa\nbusiness quan trọng hơn.\n\n\n1 `import` `numpy as np`\n\n2 `import` `pandas as pd`\n\n\n3\n\n\n4 `def` `remove_correlated_features (X, threshold =0.8):`\n\n\n5 `\"\"\"`\n\n\n6 `Remove` `features` `with high` `correlation`\n\n\n7\n\n\n8 `Parameters:`\n\n\n9 `-----------`\n\n\n10 `X : array -like`\n\n\n45"
        },
        {
          "idx": 6,
          "page": 47,
          "score": 0.6565414071083069,
          "text": "nflation Factor)**\n\n\nVIF đo mức độmột feature bịdựđoán bởi các features khác."
        }
      ],
      "model_verdict": {
        "supported": false,
        "confidence": 0.9,
        "evidence": "VIF đo mức độ một feature bị dự đoán bởi các features khác.",
        "reason": "Context không cung cấp thông tin về việc VIF-based Removal có nền tảng thống kê và sử dụng ngưỡng khách quan, nên không thể chứng thực đáp án."
      }
    },
    "2": {
      "supported_by_embeddings": true,
      "max_similarity": 0.9880712032318115,
      "evidence": [
        {
          "idx": 8,
          "page": 15,
          "score": 0.9730517864227295,
          "text": "- Nếu KNN cho kết quảquá thấp, ta có thể **phân tích nguyên nhân** (ví dụ: dữliệu có nhiễu,\nphân bốkhông đồng đều, giá trịngoại lai,...) đểhiểu rõ hơn vềdataset. Sau đó, ta có thểchọn\nmột phương pháp khác phù hợp hơn. **Tóm lại:** KNN Regression là một cách tiếp cận đơn giản, không đòi hỏi giảđịnh vềdữliệu, và giúp\nđánh giá tổng quan vềdataset. Tuy nhiên, nếu KNN không hoạt động tốt, ta cần kiểm tra lại dữliệu\nvà cân nhắc phương pháp khác. **6** **Tăng tốc KNN bằng KD-Tree**\n\n\n**Giải thích vềhàm** `fit()` **trong KNN**\n\n\nKhi sửdụng KNN, ta có đoạn code:\n\n\n1 `knn.fit(X_train, y_train)`\n\n2 `pred_i = knn.predict(X_test)`\n\n\nTa biết rằng KNN không thực sựcó giai đoạn \"train\"giống như các mô hình học máy khác (như\nLinear Regression hay Neural Networks). Vậy tại sao vẫn có hàm `fit()`, và nó có thực sự\"train\"mô\nhình không? **Thực tế**, hàm `fit()` trong KNN **không** thực hiện quá trình học (training) mà chỉlưu trữdữliệu\nhuấn luyện và **xây dựng một cấu trúc dữliệu hỗtrợ** đểtăng tốc quá trìn..."
        },
        {
          "idx": 113,
          "page": 15,
          "score": 0.9730517864227295,
          "text": "- Nếu KNN cho kết quảquá thấp, ta có thể **phân tích nguyên nhân** (ví dụ: dữliệu có nhiễu,\nphân bốkhông đồng đều, giá trịngoại lai,...) đểhiểu rõ hơn vềdataset. Sau đó, ta có thểchọn\nmột phương pháp khác phù hợp hơn. **Tóm lại:** KNN Regression là một cách tiếp cận đơn giản, không đòi hỏi giảđịnh vềdữliệu, và giúp\nđánh giá tổng quan vềdataset. Tuy nhiên, nếu KNN không hoạt động tốt, ta cần kiểm tra lại dữliệu\nvà cân nhắc phương pháp khác. **6** **Tăng tốc KNN bằng KD-Tree**\n\n\n**Giải thích vềhàm** `fit()` **trong KNN**\n\n\nKhi sửdụng KNN, ta có đoạn code:\n\n\n1 `knn.fit(X_train, y_train)`\n\n2 `pred_i = knn.predict(X_test)`\n\n\nTa biết rằng KNN không thực sựcó giai đoạn \"train\"giống như các mô hình học máy khác (như\nLinear Regression hay Neural Networks). Vậy tại sao vẫn có hàm `fit()`, và nó có thực sự\"train\"mô\nhình không? **Thực tế**, hàm `fit()` trong KNN **không** thực hiện quá trình học (training) mà chỉlưu trữdữliệu\nhuấn luyện và **xây dựng một cấu trúc dữliệu hỗtrợ** đểtăng tốc quá trìn..."
        },
        {
          "idx": 154,
          "page": 24,
          "score": 0.9880712032318115,
          "text": "Một hệquảcủa lời nguyền chiều dữliệu là **hầu hết các điểm dữliệu**\n**nằm rất gần các siêu phẳng phân loại** _→_ chỉcần nhiễu nhỏ(thường khó nhận ra) là có thểđổi\nnhãn phân loại. Hiện tượng này chính là _mẫu đối kháng_ ( _adversarial samples_ ). **7.1.3 Xửlý Dữliệu nhiều chiều**\n\n\nXửlý dữliệu có kích thước cao không phải là một nhiệm vụdễdàng. Kích thước cao có nghĩa là chúng\nta cần lưu trữcác vectơ hoặc mảng với sốlượng phần tử/sốlượng lớn, điều này tiêu tốn một lượng lớn\nbộnhớlưu trữ. Không chỉbộnhớlưu trữ, mà các thao tác tìm kiếm trong không gian kích thước cao\ncũng là một nút thắt cổchai, vì thời gian và tính toán tăng theo cấp sốnhân. Do đó, việc giảm kích\nthước dữliệu là rất quan trọng, nhưng khi giảm kích thước, chúng ta cũng cần giữlại được phân bố\nhoặc các mẫu biểu hiện trong không gian kích thước cao. **7.1.4 Giảm Kích Thước: PCA, LDA, t-SNE**\n\n\nCác kỹthuật giảm kích thước biến đổi dữliệu có chiều cao thành không gian có chiều thấp hơn, đồng\nthời bảo tồn các cấu trúc quan tr..."
        }
      ],
      "model_verdict": {
        "supported": false,
        "confidence": 0.9,
        "evidence": "",
        "reason": "Trong ngữ cảnh không có đề cập đến RobustScaler hay bất kỳ bước nào trong pipeline dùng để giảm ảnh hưởng của outliers"
      }
    },
    "3": {
      "supported_by_embeddings": true,
      "max_similarity": 0.8513287305831909,
      "evidence": [
        {
          "idx": 104,
          "page": 29,
          "score": 0.8513287305831909,
          "text": "Tuy nhiên, khi một lớp hiếm, hầu hết các hàng xóm đó có thể\nđến từlớp chiếm ưu thế- tạo ra sựthiên vịtrong kết quả. 28"
        }
      ],
      "model_verdict": {
        "supported": true,
        "confidence": 0.99,
        "evidence": "SMOTE ... tạo ra các điểm thiểu số tổng hợp mới bằng cách nội suy giữa các điểm hiện có. Ý tưởng cơ bản: đối với mỗi mẫu thiểu số, chọn một trong k hàng xóm thiểu số gần nhất của nó và tạo một điểm ở giữa.",
        "reason": "Context explicitly describes SMOTE generating synthetic minority points by interpolating between a minority sample and one of its nearest minority neighbors."
      }
    },
    "4": {
      "supported_by_embeddings": true,
      "max_similarity": 0.7898242473602295,
      "evidence": [
        {
          "idx": 78,
          "page": 13,
          "score": 0.7718899250030518,
          "text": "**AI VIETNAM** **aivietnam.edu.vn**\n\n\n  - **Tại sao k chẵn ảnh hưởng đến độchính xác?**\nĐểhiểu điều này, ta xem lại các bước của KNN:\n\n\n1. Chọn sốlượng hàng xóm k. 2. Định nghĩa phương pháp đo độtương đồng (thường là Euclidean distance). 3. Tìm **k hàng xóm gần nhất** . 4. **Voting đểdựđoán nhãn của điểm mới** . Ở **bước 4 (Voting)**, nếu k là sốchẵn, có thểxảy ra trường hợp **sốlượng nhãn bịchia đều** giữa\nhai lớp. Ví dụ: - Với **k = 4**, nếu 2 điểm thuộc loại A và 2 điểm thuộc loại B, thuật toán sẽkhông biết\nchọn loại nào, dẫn đến sựbất ổn định trong dựđoán. - Tương tự, trong bài toán phân loại nhiều lớp\nnhư **Iris dataset**, giá trịk không phù hợp có thểlàm giảm hiệu suất mô hình. **4.3** **Cách giải quyết vấn đềkhi k là sốchẵn?**\n\n\nNếu chỉsửdụng voting, việc chọn loại A hay B đều hợp lý, nhưng điều này không tối ưu. Thay vào đó,\nta có thể **đánh trọng số(weighting)** đểưu tiên các điểm gần hơn trong không gian đặc trưng. **Ba phương pháp đánh trọng sốtrong KNN:**\n\n\n1. **Uniform wei..."
        },
        {
          "idx": 199,
          "page": 13,
          "score": 0.7718899250030518,
          "text": "**AI VIETNAM** **aivietnam.edu.vn**\n\n\n  - **Tại sao k chẵn ảnh hưởng đến độchính xác?**\nĐểhiểu điều này, ta xem lại các bước của KNN:\n\n\n1. Chọn sốlượng hàng xóm k. 2. Định nghĩa phương pháp đo độtương đồng (thường là Euclidean distance). 3. Tìm **k hàng xóm gần nhất** . 4. **Voting đểdựđoán nhãn của điểm mới** . Ở **bước 4 (Voting)**, nếu k là sốchẵn, có thểxảy ra trường hợp **sốlượng nhãn bịchia đều** giữa\nhai lớp. Ví dụ: - Với **k = 4**, nếu 2 điểm thuộc loại A và 2 điểm thuộc loại B, thuật toán sẽkhông biết\nchọn loại nào, dẫn đến sựbất ổn định trong dựđoán. - Tương tự, trong bài toán phân loại nhiều lớp\nnhư **Iris dataset**, giá trịk không phù hợp có thểlàm giảm hiệu suất mô hình. **4.3** **Cách giải quyết vấn đềkhi k là sốchẵn?**\n\n\nNếu chỉsửdụng voting, việc chọn loại A hay B đều hợp lý, nhưng điều này không tối ưu. Thay vào đó,\nta có thể **đánh trọng số(weighting)** đểưu tiên các điểm gần hơn trong không gian đặc trưng. **Ba phương pháp đánh trọng sốtrong KNN:**\n\n\n1. **Uniform wei..."
        },
        {
          "idx": 71,
          "page": 31,
          "score": 0.7898242473602295,
          "text": "SMOTE (Synthetic Minority\nOver-sampling Technique) tạo ra các điểm thiểu sốtổng hợp mới bằng cách nội suy giữa các điểm hiện\ncó. Ý tưởng cơ bản: đối với mỗi mẫu thiểu số, chọn một trong k hàng xóm thiểu sốgần nhất của nó và\ntạo một điểm ởgiữa. 1 `from` `imblearn.over_sampling` `import` `SMOTE`\n\n2 `sm = SMOTE(random_state =42)`\n\n3 `X_res, y_res = sm.fit_resample(X_train, y_train)`\n\n4 `print (\" Before: \", sorted(Counter(y_train).items ()))`\n\n5 `print (\" After: \", sorted(Counter(y_res).items ()))`\n\n\n**Khi nào sửdụng:** SMOTE phù hợp khi bạn có đủmẫu thiểu sốgốc đểnội suy có ý nghĩa là có\nthể. Nó giảđịnh lớp thiểu sốcó phần nào liên tục trong không gian đặc trưng. **Mẹo triển khai:** Sửdụng imblearn trong một pipeline đểtránh rò rỉdữliệu:\n\n\n1 `from` `imblearn.pipeline` `import` `Pipeline`\n\n2 `pipe = Pipeline ([`\n\n3 `(’smote ’, SMOTE(random_state =42)),`\n\n4 `(’knn ’, KNeighborsClassifier (n_neighbors =5))`\n\n5 `])`\n\n6 `pipe.fit(X_train, y_train)`\n\n\n**7.2.4 Giảm mẫu Lớp đa số**\n\n\nĐôi khi việc..."
        }
      ],
      "model_verdict": {
        "supported": true,
        "confidence": 0.9,
        "evidence": "thay vào đó, ta có thể **đánh trọng số(weighting)** để ưu tiên các điểm gần hơn trong không gian đặc trưng.",
        "reason": "Context đề cập đến việc sử dụng trọng số (weighting) trong kNN để ưu tiên các điểm gần hơn, tức là bỏ phiếu kNN có trọng số khoảng cách, nên đáp án được chứng thực."
      }
    },
    "5": {
      "supported_by_embeddings": true,
      "max_similarity": 0.766804039478302,
      "evidence": [
        {
          "idx": 8,
          "page": 15,
          "score": 0.766804039478302,
          "text": "- Nếu KNN cho kết quảquá thấp, ta có thể **phân tích nguyên nhân** (ví dụ: dữliệu có nhiễu,\nphân bốkhông đồng đều, giá trịngoại lai,...) đểhiểu rõ hơn vềdataset. Sau đó, ta có thểchọn\nmột phương pháp khác phù hợp hơn. **Tóm lại:** KNN Regression là một cách tiếp cận đơn giản, không đòi hỏi giảđịnh vềdữliệu, và giúp\nđánh giá tổng quan vềdataset. Tuy nhiên, nếu KNN không hoạt động tốt, ta cần kiểm tra lại dữliệu\nvà cân nhắc phương pháp khác. **6** **Tăng tốc KNN bằng KD-Tree**\n\n\n**Giải thích vềhàm** `fit()` **trong KNN**\n\n\nKhi sửdụng KNN, ta có đoạn code:\n\n\n1 `knn.fit(X_train, y_train)`\n\n2 `pred_i = knn.predict(X_test)`\n\n\nTa biết rằng KNN không thực sựcó giai đoạn \"train\"giống như các mô hình học máy khác (như\nLinear Regression hay Neural Networks). Vậy tại sao vẫn có hàm `fit()`, và nó có thực sự\"train\"mô\nhình không? **Thực tế**, hàm `fit()` trong KNN **không** thực hiện quá trình học (training) mà chỉlưu trữdữliệu\nhuấn luyện và **xây dựng một cấu trúc dữliệu hỗtrợ** đểtăng tốc quá trìn..."
        }
      ],
      "model_verdict": {
        "supported": true,
        "confidence": 0.99,
        "evidence": "Giả sủ trong 1000 bệnh nhân, chỉ có 5 người thực sự bị ung thư... Giải thuật 1 (Sai nhưng có Accuracy cao) trả về 0 và vẫn có accuracy = 99%",
        "reason": "Context nêu rõ dataset mất cân bằng với đa số bệnh nhân không mắc ung thư, vì vậy thuật toán luôn trả về 0 vẫn đạt accuracy cao."
      }
    }
  }
}